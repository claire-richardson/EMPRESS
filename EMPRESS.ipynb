{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2589d2-85e3-4cdb-9509-6cd238db3402",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Earth Modeling PRioritized Event Selection Scheme (EMPRESS)\n",
    "\n",
    "Developed by Claire Richardson (Arizona State University), Fall 2024\n",
    "\n",
    "In collaboration with Ed Garnero (Arizona State University) and Ebru Bozdag (Colorado School of Mines)\n",
    "\n",
    "For detailed usage notes, refer to README.md (https://github.com/claire-richardson/EMPRESS.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6d451-f4d0-4232-8106-6931b2cf8ca1",
   "metadata": {},
   "source": [
    "-----\n",
    "## PART 1: Define the modeling region of interest\n",
    "Use Part 1 in conjunction with `box_sampling_input.py` to visualize and define a 3D box around the region of interest. From usage notes:\n",
    "\n",
    "<blockquote>\n",
    "define the following variables in `box_sampling_input.py`:\n",
    "\n",
    "- `width` - longitudinal extent of the box in degrees\n",
    "- `height` - latitudinal extent of the box in degrees\n",
    "- `south` - latitude that defines the southernmost extent of the box in degrees \n",
    "- `west` - longitude that defines the westernmost extent of the box in degrees \n",
    "- `box_depth_min` - depth that defines the top of the box in km\n",
    "- `box_depth_max` - depth that defines the bottom/base of the box in km\n",
    "\n",
    "and then run `PART 1` of `EMPRESS.ipynb`. If the box doesn't cover the target region appropriately, update `box_sampling_input.py` and rerun `PART 1`. Do this until a satisfactory box has been defined.\n",
    "</blockquote>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c4a5e-1e01-4249-9218-705dc7c6fb7d",
   "metadata": {},
   "source": [
    "#### Import initial modules and define plotting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9bb6c-d7a0-425c-bcb1-fda560c7bcca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import geo_math\n",
    "import refmodels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import box_sampling_input\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def convert_azimuth(az):\n",
    "    '''\n",
    "    Function to convert azimuth range from 0 - 360 degrees to 0 - 180 degrees\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `az`: azimuth [format: float or int; unit: degrees]\n",
    "    Outputs:\n",
    "    - converted azimuth [format: float or int; unit: degrees]\n",
    "    ======\n",
    "    '''\n",
    "    if az >= 180.:\n",
    "        return az - 180.\n",
    "    else:\n",
    "        return az\n",
    "\n",
    "def find_cloud_center(lats, lons):\n",
    "    '''\n",
    "    Function to find the geographical center of a cloud of coordinates\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `lats`: 1D array of latitudes of the points in the cloud [format: 1D array-like; unit: degrees]\n",
    "    - `lons`: 1D array of longitudes of the points in the cloud [format: 1D array-like; unit: degrees]\n",
    "    Outputs:\n",
    "    - latitude and longitude of geographical cloud center [format: list of two floats; unit: degrees]\n",
    "    ======\n",
    "    '''\n",
    "    lat_rad = np.radians(lats)\n",
    "    lon_rad = np.radians(lons)\n",
    "    cloud_x = np.mean(np.cos(lon_rad) * np.cos(lat_rad))\n",
    "    cloud_y = np.mean(np.sin(lon_rad) * np.cos(lat_rad))\n",
    "    cloud_z = np.mean(np.sin(lat_rad))\n",
    "\n",
    "    cloud_lat = np.degrees(np.arcsin(cloud_z / 1.))\n",
    "    cloud_lon = np.degrees(np.arctan2(cloud_y, cloud_x))\n",
    "    \n",
    "    if cloud_lon >= 180.:\n",
    "        cloud_lon -= 360.\n",
    "    if cloud_lon < -180:\n",
    "        cloud_lon += 360\n",
    "\n",
    "    return [cloud_lat, cloud_lon]\n",
    "\n",
    "def dist_from_point(start_lat, start_lon, end_lats, end_lons):\n",
    "    '''\n",
    "    Function to find the epicentral distance from one starting point to an array of end points\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `start_lat`: latitude of the starting point [format: float or int; unit: degrees]\n",
    "    - `start_lon`: longitude of the starting point [format: float or int; unit: degrees]\n",
    "    - `end_lats`: 1D array of latitudes of the end points [format: 1D array-like; unit: degrees]\n",
    "    - `end_lons`: 1D array of longitudes of the end points [format: 1D array-like; unit: degrees]\n",
    "    Outputs:\n",
    "    - a 1D array of epicentral distances from the starting point to the end points [format: 1D array-like; unit: degrees]\n",
    "    ======\n",
    "    '''\n",
    "    start_lat_rad = np.radians(start_lat)\n",
    "    start_lon_rad = np.radians(start_lon + 180.)\n",
    "    end_lats_rad = np.radians(end_lats)\n",
    "    end_lons_rad = np.radians(end_lons + 180.)\n",
    "    \n",
    "    a = (np.sin((end_lats_rad - start_lat_rad) / 2) ** 2) + np.cos(start_lat_rad) * np.cos(end_lats_rad) * (np.sin((end_lons_rad - start_lon_rad) / 2) ** 2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return np.degrees(c)\n",
    "    \n",
    "def az_from_point(start_lat, start_lon, end_lats, end_lons):\n",
    "    '''\n",
    "    Function to find the azimuths from one starting point to an array of end points\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `start_lat`: latitude of the starting point [format: float or int; unit: degrees]\n",
    "    - `start_lon`: longitude of the starting point [format: float or int; unit: degrees]\n",
    "    - `end_lats`: 1D array of latitudes of the end points [format: 1D array-like; unit: degrees]\n",
    "    - `end_lons`: 1D array of longitudes of the end points [format: 1D array-like; unit: degrees]\n",
    "    Outputs:\n",
    "    - a 1D array of azimuths from the starting point to the end points [format: 1D array-like; unit: degrees]\n",
    "    ======\n",
    "    '''\n",
    "    delta_lat = end_lats - start_lat\n",
    "    delta_lon = end_lons - start_lon\n",
    "        \n",
    "    start_lat_rad = np.radians(start_lat)\n",
    "    start_lon_rad = np.radians(start_lon)\n",
    "    end_lats_rad = np.radians(end_lats)\n",
    "    end_lons_rad = np.radians(end_lons)\n",
    "        \n",
    "    delta_lat_rad = np.radians(delta_lat)\n",
    "    delta_lon_rad = np.radians(delta_lon)\n",
    "    \n",
    "    az_rad = np.arctan2((np.sin(delta_lon_rad) * np.cos(end_lats_rad)), (np.cos(start_lat_rad) * np.sin(end_lats_rad) - np.sin(start_lat_rad) * np.cos(end_lats_rad) * np.cos(delta_lon_rad)))\n",
    "    az = np.degrees(az_rad)\n",
    "    \n",
    "    az = az.mask((az < 0.), (360 - abs(az)))\n",
    "    az = az.mask((az == 360), 0)\n",
    "    \n",
    "    return az\n",
    "\n",
    "def az_diff(az1, az2):\n",
    "    '''\n",
    "    Function to find the absolute (i.e., <= 180 degrees) angular difference between a starting azimuth and an array of other azimuths\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `az1`: starting azimuth [format: float or int; unit: degrees]\n",
    "    - `az2`: 1D array of other azimuths [format: 1D array-like; unit: degrees]\n",
    "    Outputs:\n",
    "    - a 1D array of angular differences between the starting azimuth and azimuths in az2 [format: 1D array-like; unit: degrees]\n",
    "    ======\n",
    "    '''\n",
    "    az_diff_series = abs(az1 - az2)\n",
    "    az_diff_series = az_diff_series.mask((az_diff_series > 180), (360 - az_diff_series))\n",
    "\n",
    "    return az_diff_series\n",
    "\n",
    "def find_nearest(array, value, type):\n",
    "    '''\n",
    "    Function to find the nearest value or values in an array to a given value\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `array`: array of values to query for the closest value [format: 1D array-like; unit: numeric]\n",
    "    - `value`: given value to find the closest `array` values [format: float or int; unit: numeric]\n",
    "    - `type`: type of closest value to find, either 'absolute' (i.e., find the singular closest absolute value) or 'bounds' (i.e., find the closest values on either side of the given value [format: string]\n",
    "    Outputs:\n",
    "    - either one index (`type` == 'absolute') or two indices (`type` == 'bounds') of the closest points in `array` to `value` [format: int; unit: index]\n",
    "    ======\n",
    "    '''\n",
    "    if type == 'absolute':\n",
    "        array_diff = abs(array - value)\n",
    "        id = np.where(array_diff == array_diff.min())[0][0]\n",
    "        return [id]\n",
    "    \n",
    "    elif type == 'bounds':\n",
    "        array_diff = array - value\n",
    "        smallest_negative_value = array_diff[np.where(array_diff <= 0)].max()\n",
    "        smallest_positive_value = array_diff[np.where(array_diff >= 0)].min()\n",
    "        negative_id = np.where(array_diff == smallest_negative_value)[0][0]\n",
    "        positive_id = np.where(array_diff == smallest_positive_value)[0][0]\n",
    "        return [negative_id, positive_id]\n",
    "\n",
    "def lateral_grid_register(df_model, model_wave_type, model_depth, coord_increment, column_name, model_value_type):\n",
    "    '''\n",
    "    Function to convert a tomography model CSV into a plottable gridded array at a given depth. Currently supports models given in perturbations relative to a reference model, or velocities from models made relative to PREM (Dziewonski & Anderson, 1981)\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `df_model` - pandas dataframe of the whole tomography model (model file should be stored as /EMPRESS/models/{model}.csv) [format: Pandas dataframe]\n",
    "    - `model_wave_type` - wave polarity to plot (either 'S' or 'P') [format: string]\n",
    "    - `model_depth` - depth at which to plot the model [format: int or float; unit: km]\n",
    "    - `coord_increment` - grid spacing / resolution [format: int; unit: degrees]\n",
    "    - `column_name` - header name in pandas dataframe of the value to plot [format: string]\n",
    "    - `model_value_type` - type of velocity value in `column_name`, either 'velocity' if model values are given in km/s, or 'perturbation' if model values are given as percent perturbations from a reference model [format: string]\n",
    "    Outputs:\n",
    "    - plottable gridded 2D array of model values [format: 2D numpy array; unit: `column_name` units]\n",
    "    ======\n",
    "    '''\n",
    "    prem_vel = refmodels.prem_vel(model_wave_type, model_depth)\n",
    "    model_depths = df_model.depth.unique()\n",
    "    array_diff = abs(model_depths - model_depth)\n",
    "    id = np.where(array_diff == array_diff.min())[0][0]\n",
    "    ar_model_depth = np.array(df_model.loc[df_model['depth'] == model_depths[id]][['latitude', 'longitude', column_name]].reset_index(drop = True))\n",
    "    \n",
    "    reg_lats = np.arange(-90, 90, coord_increment)\n",
    "    reg_lons = np.arange(-180, 180, coord_increment)\n",
    "    ar = np.zeros((len(reg_lons), len(reg_lats)))\n",
    "\n",
    "    lat_idx = 0\n",
    "    for lat in reg_lats:\n",
    "        lat_diff = abs(ar_model_depth.T[0] - lat)\n",
    "        ar_closest_lat = ar_model_depth[np.where(lat_diff == lat_diff.min())[0]]\n",
    "\n",
    "        lon_idx = 0\n",
    "        for lon in reg_lons:\n",
    "            lon_diff = abs(ar_closest_lat.T[1] - lon)\n",
    "            closest_id = np.where(lon_diff == lon_diff.min())[0][0]\n",
    "            model_value = ar_closest_lat.T[-1, closest_id]\n",
    "\n",
    "            if model_value_type == 'velocity':\n",
    "                if model_value == 0.:\n",
    "                    model_value = 0.\n",
    "                else:\n",
    "                    model_value = ((model_value / prem_vel) * 100.) - 100.\n",
    "                    \n",
    "            ar[lon_idx, lat_idx] = model_value\n",
    "            lon_idx += 1\n",
    "            \n",
    "        lat_idx += 1\n",
    "\n",
    "    ar = ar.T\n",
    "    return ar\n",
    "\n",
    "def radial_grid_register(df_model, model_wave_type, start_lat, start_lon, end_lat, end_lon, coord_increment, column_name, model_value_type):\n",
    "    '''\n",
    "    Function to convert a tomography model CSV into a plottable gridded array along a given cross-section. Currently supports models given in perturbations relative to a reference model, or velocities from models made relative to PREM (Dziewonski & Anderson, 1981)\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `df_model` - pandas dataframe of the whole tomography model (model file should be stored as /EMPRESS/models/{model}.csv) [format: Pandas dataframe]\n",
    "    - `model_wave_type` - wave polarity to plot (either 'S' or 'P') [format: string]\n",
    "    - `start_lat` - starting latitude of the cross-section [format: float or int; unit: degrees]\n",
    "    - `start_lon` - starting longitude of the cross-section [format: float or int; unit: degrees]\n",
    "    - `end_lat` - ending latitude of the cross-section [format: float or int; unit: degrees]\n",
    "    - `end_lon` - ending longitude of the cross-section [format: float or int; unit: degrees]\n",
    "    - `coord_increment` - grid spacing / resolution [format: int; unit: degrees]\n",
    "    - `column_name` - header name in pandas dataframe of the value to plot [format: string]\n",
    "    - `model_value_type` - type of velocity value in `column_name`, either 'velocity' if model values are given in km/s, or 'perturbation' if model values are given as percent perturbations from a reference model [format: string]\n",
    "    Outputs:\n",
    "    - plottable gridded 2D array of model values [format: 2D numpy array; unit: `column_name` units]\n",
    "    ======\n",
    "    '''\n",
    "    ar_model = np.array(df_model[['depth', 'latitude', 'longitude', column_name]])\n",
    "    model_depths = list(df_model.depth.unique())\n",
    "    x_section_length = geo_math.GCP_length(start_lat, start_lon, end_lat, end_lon)\n",
    "    x_section_increments = int(x_section_length / coord_increment)\n",
    "    x_section_increment_length =  x_section_length / x_section_increments\n",
    "    ar = np.zeros((len(model_depths), x_section_increments + 1))\n",
    "    \n",
    "    depth_idx = 0\n",
    "    for model_depth in model_depths:\n",
    "\n",
    "        ar_depth = ar_model[np.where(ar_model.T[0] == model_depth)[0]]\n",
    "        prem_vel = refmodels.prem_vel(model_wave_type, model_depth)\n",
    "    \n",
    "        total_dist_along_path = 0\n",
    "        inc_idx = 0\n",
    "        for increment in range(x_section_increments + 1):\n",
    "            # find the closest model value to each incremental point\n",
    "            # first, find the incremental point\n",
    "            point_along_path = geo_math.GCP_point(start_lat, start_lon, end_lat, end_lon, x_section_length, total_dist_along_path)\n",
    "            \n",
    "            # then, find the nearest model value\n",
    "            lat_diff = abs(ar_depth.T[1] - point_along_path[0])\n",
    "            ar_closest_lat = ar_depth[np.where(lat_diff == lat_diff.min())[0]]\n",
    "    \n",
    "            lon_diff = abs(ar_closest_lat.T[2] - point_along_path[1])\n",
    "            closest_pt = ar_closest_lat[np.where(lon_diff == lon_diff.min())[0][0]]\n",
    "    \n",
    "            model_value = closest_pt[-1]\n",
    "\n",
    "            if model_value_type == 'velocity':\n",
    "                if model_value == 0.:\n",
    "                    model_value = 0.\n",
    "                else:\n",
    "                    model_value = ((model_value / prem_vel) * 100.) - 100.\n",
    "    \n",
    "            ar[depth_idx, inc_idx] = model_value\n",
    "            total_dist_along_path += x_section_increment_length\n",
    "            inc_idx += 1\n",
    "            \n",
    "        depth_idx += 1\n",
    "    \n",
    "    return ar\n",
    "\n",
    "def make_equal_area_grid(side_length, start_lat, start_lon, end_lat, end_lon):\n",
    "    '''\n",
    "    Function to make a lateral grid of approximate equal area blocks\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `side_length` - side lengths of the reference grid block at the equator. Must be divisible into both the total latitudinal and longitudinal extent of the lateral area to be gridded [format: int; unit: degrees]\n",
    "    - `start_lat` - starting latitude of the lateral area to be gridded (southernmost latitude) [format: float or int; unit: degrees]\n",
    "    - `start_lon` - starting longitude of the lateral area to be gridded (westernmost longitude) [format: float or int; unit: degrees]\n",
    "    - `end_lat` - ending latitude of the lateral area to be gridded (northernmost latitude) [format: float or int; unit: degrees]\n",
    "    - `end_lon` - ending longitude of the lateral area to be gridded (easternmost longitude) [format: float or int; unit: degrees]\n",
    "    Outputs:\n",
    "    - pandas dataframe of approximate equal area 2D grid blocks for the lateral area defined by `start_lat`, `start_lon`, `end_lat`, and `end_lon` with approximate equal area block dimensions\n",
    "    ======\n",
    "    '''\n",
    "    # presets:\n",
    "    rad = 6371 # radius\n",
    "    latitude = []\n",
    "    circumference_per_lat = []\n",
    "    \n",
    "    if (start_lon > 0.) and (end_lon < 0.):\n",
    "        total_lon = (180 - start_lon) + (180 + end_lon)\n",
    "    else:\n",
    "        total_lon = end_lon - start_lon\n",
    "    \n",
    "    total_lat = start_lat - end_lat\n",
    "    lon_ratio = total_lon / 360\n",
    "    \n",
    "    # circumference at equator:\n",
    "    eq_circ = 2 * np.pi * rad\n",
    "\n",
    "    # gridblock area:\n",
    "    area = (np.pi/180) * rad**2 * (np.sin(np.deg2rad(side_length)) - np.sin(np.deg2rad(0))) * (side_length)\n",
    "    \n",
    "    # degrees of lon in km at the equator:\n",
    "    deg_in_km = eq_circ / 360\n",
    "    \n",
    "    # circumference at other latitudes:\n",
    "    for lat in range(start_lat, end_lat, side_length):\n",
    "        latitude.append(lat)\n",
    "        absolute_lat = np.absolute(lat)\n",
    "        if lat != 0:\n",
    "            scale_lat = 90 - absolute_lat\n",
    "        if lat == 0:\n",
    "            scale_lat = 90\n",
    "        scale_factor = scale_lat / 90\n",
    "        circ = scale_factor * eq_circ\n",
    "        circumference_per_lat.append(circ)\n",
    "    \n",
    "    # block info:\n",
    "    df_block = pd.DataFrame(data = {'LATITUDE': latitude, 'CIRC': circumference_per_lat})\n",
    "    df_block['BAND_AREA'] = (rad ** 2) * (np.sin(np.deg2rad(df_block['LATITUDE'] + side_length)) - np.sin(np.deg2rad(df_block['LATITUDE']))) * (np.deg2rad(total_lon))\n",
    "    df_block['#_OF_BLOCKS'] = df_block['BAND_AREA'] / area\n",
    "    df_block['#_OF_ROUND_BLOCKS'] = df_block['#_OF_BLOCKS'].round(decimals = 0)\n",
    "    df_block['#_OF_ROUND_BLOCKS'] = df_block['#_OF_ROUND_BLOCKS'].astype(int)\n",
    "    df_block['BLOCK_AREA'] = df_block['BAND_AREA'] / df_block['#_OF_ROUND_BLOCKS']\n",
    "    \n",
    "    # block file output:\n",
    "    block_number = []\n",
    "    lat_band_id = []\n",
    "    lat_min = []\n",
    "    lat_max = []\n",
    "    lon_min = []\n",
    "    lon_max = []\n",
    "    center_lat = []\n",
    "    center_lon = []\n",
    "    \n",
    "    # initialize lat band id\n",
    "    lat_band = 1\n",
    "    ref_start_lat = start_lat\n",
    "    \n",
    "    # effectively, for each latitude band:\n",
    "    for line in range(len(df_block)):\n",
    "        number_of_blocks = df_block['#_OF_ROUND_BLOCKS'].iloc[line]\n",
    "        lon_division = total_lon / number_of_blocks\n",
    "        ref_start_lon = start_lon\n",
    "    \n",
    "        for num in range(number_of_blocks):\n",
    "            lat_min.append(ref_start_lat)\n",
    "            lat_max.append(ref_start_lat + side_length)\n",
    "            center_lat.append(ref_start_lat + (side_length / 2))\n",
    "            lat_band_id.append(lat_band)\n",
    "    \n",
    "            lon_min.append(ref_start_lon)\n",
    "            lon_max.append(ref_start_lon + lon_division)\n",
    "            center_lon.append(ref_start_lon + (lon_division / 2))\n",
    "    \n",
    "            ref_start_lon += lon_division\n",
    "            if ref_start_lon >= 180.:\n",
    "                ref_start_lon -= 360\n",
    "    \n",
    "        lat_band += 1\n",
    "        ref_start_lat += side_length\n",
    "    \n",
    "    block_dim_data = {'BLOCK#': list(range(1, len(lat_min) + 1)), 'LAT_BAND_ID': lat_band_id, 'LAT_MIN': lat_min, 'LAT_MAX': lat_max, 'LON_MIN': lon_min, 'LON_MAX': lon_max, 'CENTER_LAT': center_lat, 'CENTER_LON': center_lon}\n",
    "    df_block_dim = pd.DataFrame(data = block_dim_data)\n",
    "    df_block_dim['LON_MAX'].mask((df_block_dim['LON_MAX'] >= 180.), (df_block_dim['LON_MAX'] - 360.), inplace = True)\n",
    "    return df_block_dim\n",
    "\n",
    "def find_block_id(lat, lon, ar_blocks):\n",
    "    '''\n",
    "    Function to find the block ID that a given latitude and longitude pair belong to for approximate equal area grid computed in function `make_equal_area_grid`\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `lat`: latitude [format: float or int; unit: degrees]\n",
    "    - `lon`: longitude [format: float or int; unit: degrees]\n",
    "    - `ar_blocks`: numpy array object of approximate equal area 2D blocks (generated as a pandas dataframe from `make_equal_area_grid`) [format: 2D numpy array]\n",
    "    Outputs:\n",
    "    - block ID of block in which `(lat1, lon1)` falls [format: int]\n",
    "    ======\n",
    "    '''\n",
    "    ar_blocks_t = ar_blocks.T\n",
    "    if lon >= 180.:\n",
    "        lon -= 360.\n",
    "        \n",
    "    if lat != 90.:\n",
    "        block_idx = np.where((ar_blocks_t[2] <= lat) & (ar_blocks_t[3] > lat) & (ar_blocks_t[4] <= lon) & (ar_blocks_t[5] > lon))[0]\n",
    "        if len(block_idx) == 0:\n",
    "            block_id = np.where((ar_blocks_t[2] <= lat) & (ar_blocks_t[3] > lat) & (ar_blocks_t[4] > 0) & (ar_blocks_t[5] < 0))[0][0] + 1\n",
    "        else:\n",
    "            block_id = np.where((ar_blocks_t[2] <= lat) & (ar_blocks_t[3] > lat) & (ar_blocks_t[4] <= lon) & (ar_blocks_t[5] > lon))[0][0] + 1\n",
    "    \n",
    "    elif lat == 90.:\n",
    "        block_idx = np.where((ar_blocks_t[3] == 90.) & (ar_blocks_t[4] <= lon) & (ar_blocks_t[5] > lon))[0]\n",
    "        if len(block_idx) == 0:\n",
    "            block_id = np.where((ar_blocks_t[3] == 90.) & (ar_blocks_t[4] > 0) & (ar_blocks_t[5] < 0))[0][0] + 1\n",
    "        else:\n",
    "            block_id = np.where((ar_blocks_t[3] == 90.) & (ar_blocks_t[4] <= lon) & (ar_blocks_t[5] > lon))[0][0] + 1\n",
    "    return int(block_id) \n",
    "\n",
    "azimuthal_sectors = 6\n",
    "azimuthal_sector_extent = 180. / azimuthal_sectors\n",
    "df_sectors = pd.DataFrame(columns = ['sector', 'min_extent', 'max_extent'])\n",
    "sector_nos = list(range(1, azimuthal_sectors + 1))\n",
    "df_sectors['sector'] = sector_nos\n",
    "df_sectors['min_extent'] = (df_sectors['sector'] - 1) * azimuthal_sector_extent\n",
    "df_sectors['max_extent'] = df_sectors['sector'] * azimuthal_sector_extent\n",
    "df_sectors = np.array(df_sectors.T)\n",
    "\n",
    "def azimuthal_sector(az):\n",
    "    '''\n",
    "    Function to find the azimuthal sector that `az` falls in. Azimuthal sectors are defined as six 30 degree sectors that range from 0-180 degrees.\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `az`: azimuth [format: float or int; unit: degrees]\n",
    "    Outputs:\n",
    "    - azimuthal sector in which `az` falls [format: int]\n",
    "    ======\n",
    "    '''\n",
    "    if az == 360.:\n",
    "        az = 0.\n",
    "    if az >= 180.:\n",
    "        az -= 180.\n",
    "    return int(np.where((df_sectors[1] <= az) & (df_sectors[2] > az))[0][0] + 1)\n",
    "\n",
    "def coverage_mesh(ar_blocks, ar_coverage, coord_increment, column_index):\n",
    "    '''\n",
    "    Function to convert a pandas dataframe with coverage for 2D approximate equal area blocks computed in function `make_equal_area_grid` into a gridded plottable array\n",
    "    ======\n",
    "    Inputs:\n",
    "    - `ar_blocks`: numpy array object of approximate equal area 2D blocks (generated as a pandas dataframe from `make_equal_area_grid`) [format: 2D numpy array]\n",
    "    - `ar_coverage`: numpy array object of approximate equal area 2D blocks with total and azimuthal coverage information [format: 2D numpy array]\n",
    "    - `coord_increment` - grid spacing / resolution [format: int; unit: degrees]\n",
    "    - `column_index` - index of the column of values to convert in `ar_coverage`\n",
    "    Outputs:\n",
    "    - plottable gridded 2D array of coverage values [format: 2D numpy array; unit: `column_index` units]\n",
    "    ======\n",
    "    '''    \n",
    "    reg_lats = np.arange(s, n, coord_increment)\n",
    "    reg_lons = np.arange(0, box_sampling_input.width, coord_increment)\n",
    "    reg_lons = reg_lons + w\n",
    "    reg_lons[np.where(reg_lons >= 180)[0]] = reg_lons[np.where(reg_lons >= 180)[0]] - 360    \n",
    "    ar = np.zeros((len(reg_lons), len(reg_lats)))\n",
    "\n",
    "    lat_idx = 0\n",
    "    for lat in reg_lats:\n",
    "        lat_vals = []\n",
    "        lon_idx = 0\n",
    "        for lon in reg_lons:\n",
    "            block = find_block_id(lat, lon, ar_blocks)\n",
    "            value = ar_coverage[np.where(ar_coverage.T[0] == block)[0][0], column_index]\n",
    "            ar[lon_idx, lat_idx] = value\n",
    "            lon_idx += 1\n",
    "        lat_idx += 1\n",
    "    return ar.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a429873-4456-417e-8ead-96f657b4d4f6",
   "metadata": {},
   "source": [
    "#### Vizualize the focus region\n",
    "\n",
    "If you wish to plot your defined region over a global tomography model, make sure a CSV of the model is saved in `/EMPRESS/models/{model}.csv`. Global models can be downloaded as netCDF files from the SAGE EMC (https://ds.iris.edu/ds/products/emc-earthmodels/) and must be converted to CSV format using their provided tools. Currently, if models are given in veloicity (km/s), plotting is only supported for models made relative to PREM. If models are given as perturbations relative to a 1D reference model, they are plottable regardless of reference model.\n",
    "\n",
    "#### Define the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac1e03-059d-40c6-ae4d-bfeefcf40f46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global tomography model layer attributes:\n",
    "model = None # model name from '/EMPRESS/models' directory. If no model layer is desired or available for the map, define as `None`\n",
    "model_wave_type = 'S' # wave polarity of the model to plot. Either 'S', 'P'. If `model` == 'None, this variable is ignored\n",
    "model_depth = 2800 # model depth to plot on the lateral map in km. If `model` == 'None, this variable is ignored\n",
    "model_value = 'vsh' # column name of the model value to plot. If `model` == 'None, this variable is ignored\n",
    "model_value_type = 'perturbation' # format of the given value, either 'perturbation' or 'velocity'. If `model` == 'None, this variable is ignored. \n",
    "map_increment = 1 # grid increment at which to plot the maps. If `model` == 'None, this variable is ignored\n",
    "tomo_bar_val = 2 # absolute value for colorbar minimum and maximum values, given in percent. If `model` == 'None, this variable is ignored\n",
    "\n",
    "# cross section attributes (must be less than 180 degrees in extent):\n",
    "start_point = [0, 117] # start point of the cross-section, plotted as a green dot\n",
    "end_point = [-0, -67] # end point of the cross-section, plotted as a red dot\n",
    "number_of_reference_points = 3 # number of evenly spaced reference points to plot along the cross-section, plotted as yellow points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd976c1-8406-4a1c-81ac-d0e5b04ada8c",
   "metadata": {},
   "source": [
    "#### Run the following cell to vizualize the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55e658-365e-44c0-9b60-d66d784f15b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize = (12,5), constrained_layout = True)\n",
    "lats = np.arange(-90, 90, map_increment)\n",
    "lons = np.arange(-180, 180, map_increment)\n",
    "tomo_cm = 'RdBu'\n",
    "\n",
    "# define map extents\n",
    "w = box_sampling_input.west\n",
    "s = box_sampling_input.south\n",
    "e = w + box_sampling_input.width\n",
    "if e >= 180.:\n",
    "    e -= 360\n",
    "n = s + box_sampling_input.height\n",
    "if n >= 90.:\n",
    "    print(f'--  Northern extent too large, n = {n}; adjust `height` variable in `box_sampling_input.py`')\n",
    "\n",
    "map_center = w + (box_sampling_input.width / 2)\n",
    "if map_center >= 180.:\n",
    "    map_center -= 360\n",
    "\n",
    "# define x-sec extents:\n",
    "xsec_length = geo_math.GCP_length(start_point[0], start_point[1], end_point[0], end_point[1])\n",
    "reference_dot_spacing = xsec_length / (number_of_reference_points + 1)\n",
    "xsec_az = geo_math.azimuth(start_point[0], start_point[1], end_point[0], end_point[1])\n",
    "\n",
    "\n",
    "## map view setup:\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection = ccrs.Robinson(central_longitude = map_center))\n",
    "ax1.set_global()\n",
    "ax1.coastlines(alpha = 0.4)\n",
    "ax1.plot([start_point[1], end_point[1]], [start_point[0], end_point[0]], c = 'k', transform = ccrs.Geodetic(), linewidth = 1, zorder = 2)\n",
    "ax1.scatter(start_point[1], start_point[0], transform = ccrs.Geodetic(), c = 'green', marker = '.', edgecolor = 'black', s = 140, zorder = 3)\n",
    "ax1.scatter(end_point[1], end_point[0], transform = ccrs.Geodetic(), c = 'red', marker = '.', edgecolor = 'black', s = 140, zorder = 3)\n",
    "\n",
    "## cross-section view setup:\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.annotate(f'Cross-section', xy = (0.5, 1.05), ha = 'center', xycoords = 'axes fraction', fontsize = 14)\n",
    "ax2.scatter(0., 0., c = 'green', marker = '.', edgecolor = 'black', s = 140, zorder = 3, clip_on = False)\n",
    "ax2.scatter(xsec_length, 0., c = 'red', marker = '.', edgecolor = 'black', s = 140, zorder = 3, clip_on = False)\n",
    "ax2.axhline(y = 400., c = 'k', linestyle = '--', alpha = 0.25)\n",
    "ax2.axhline(y = 670., c = 'k', linestyle = '--', alpha = 0.25)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_ylim(2891, 0)\n",
    "ax2.set_xlim(0, xsec_length)\n",
    "ax2.set_ylabel('\\nDepth (km)')\n",
    "ax2.set_xlabel('Distance along cross-section (deg)')\n",
    "\n",
    "## reference dots:\n",
    "reference_dot_dist = 0\n",
    "for reference_point in range(number_of_reference_points):\n",
    "    reference_dot_dist += reference_dot_spacing\n",
    "    reference_dot_coords = geo_math.GCP_point(start_point[0], start_point[1], end_point[0], end_point[1], xsec_length, reference_dot_dist)\n",
    "    ax1.scatter(reference_dot_coords[1], reference_dot_coords[0], transform = ccrs.Geodetic(), c = 'yellow', marker = '.', edgecolor = 'black', s = 140, zorder = 3)\n",
    "    ax2.scatter(reference_dot_dist, 0, c = 'yellow', marker = '.', edgecolor = 'black', s = 140, zorder = 3, clip_on = False)\n",
    "\n",
    "## add model:\n",
    "if model != None:\n",
    "    ## map view:\n",
    "    df_model = pd.read_csv(f'./models/{model}.csv')\n",
    "    df_model = df_model.loc[df_model['depth'] >= 0]\n",
    "    depths = list(df_model.depth.unique())\n",
    "    depths.insert(0, 0)\n",
    "    mod_dvs = lateral_grid_register(df_model, model_wave_type, model_depth, map_increment, model_value, model_value_type)\n",
    "    im1 = ax1.pcolormesh(lons, lats, mod_dvs, transform = ccrs.PlateCarree(), cmap = tomo_cm, vmin = -tomo_bar_val, vmax = tomo_bar_val)\n",
    "    ax1.annotate(f'Map view ({model_depth} km depth)', xy = (0.5, 1.05), ha = 'center', xycoords = 'axes fraction', fontsize = 14)\n",
    "    suptitle = f'Box area for model {model}'\n",
    "\n",
    "    ## x-section:\n",
    "    xsec_dvs = radial_grid_register(df_model, model_wave_type, start_point[0], start_point[1], end_point[0], end_point[1], map_increment, model_value, model_value_type)\n",
    "    ax2.pcolormesh(range(0, len(xsec_dvs.T) + 1), depths, xsec_dvs, cmap = tomo_cm, vmin = -tomo_bar_val, vmax = tomo_bar_val)\n",
    "\n",
    "    ## colorbar:\n",
    "    cbar_tomo = fig.colorbar(im1, ax = [ax1, ax2], orientation = 'horizontal', shrink = 0.3, pad = 0.1, extend = 'both')\n",
    "    cbar_tomo.ax.tick_params(labelsize = 12)\n",
    "    cbar_tomo.ax.set_xlabel('dVs (%)', size = 12)\n",
    "    \n",
    "else:\n",
    "    ax1.annotate(f'Map view', xy = (0.5, 1.05), ha = 'center', xycoords = 'axes fraction', fontsize = 14)\n",
    "    zero_ar = np.zeros((1, 1))\n",
    "    ax2.pcolormesh([0, xsec_length], [0, 2891], zero_ar, cmap = 'Greys', vmin = 0, vmax = 0)\n",
    "    suptitle = f'Box area'\n",
    "\n",
    "## add box outline:\n",
    "ax1.add_patch(mpatches.Rectangle(xy = [w, s], width = box_sampling_input.width, height = box_sampling_input.height, edgecolor = 'black', fill = False, alpha = 1, transform = ccrs.PlateCarree()))\n",
    "\n",
    "\n",
    "x_pts = []\n",
    "\n",
    "# # find the coordinates of all of the boundaries from the start point\n",
    "try:\n",
    "    start_to_w = geo_math.known_lon(xsec_az, start_point[0], start_point[1], w)\n",
    "    if start_to_w[2] <= xsec_length:\n",
    "        x_pts.append(start_to_w[2])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    start_to_e = geo_math.known_lon(xsec_az, start_point[0], start_point[1], e)\n",
    "    if start_to_e[2] <= xsec_length:\n",
    "        x_pts.append(start_to_e[2])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    start_to_s = geo_math.known_lat(xsec_az, start_point[0], start_point[1], s)\n",
    "    if start_to_s[2] <= xsec_length:\n",
    "        x_pts.append(start_to_s[2])\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    start_to_n = geo_math.known_lat(xsec_az, start_point[0], start_point[1], n)\n",
    "    if start_to_n[2] <= xsec_length:\n",
    "        x_pts.append(start_to_n[2])\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "for x_pt in x_pts:\n",
    "    ax2.vlines(x_pt, box_sampling_input.box_depth_min, box_sampling_input.box_depth_max, colors = 'k')\n",
    "\n",
    "if len(x_pts) == 0:\n",
    "    ax2.axhline(box_sampling_input.box_depth_min, c = 'k')\n",
    "    ax2.axhline(box_sampling_input.box_depth_max, c = 'k')\n",
    "elif len(x_pts) == 1:\n",
    "    # find which side of the plot to draw the line to \n",
    "    if (s < start_point[0] < n) and ((w < start_point[1]) or (e > start_point[1])):\n",
    "        ax2.hlines(box_sampling_input.box_depth_min, x_pts[0], 0, colors = 'k')\n",
    "        ax2.hlines(box_sampling_input.box_depth_max, x_pts[0], 0, colors = 'k')\n",
    "    elif (s < end_point[0] < n) and ((w < end_point[1]) or (e > end_point[1])):\n",
    "        print('lj')\n",
    "        ax2.hlines(box_sampling_input.box_depth_min, x_pts[0], xsec_length, colors = 'k')\n",
    "        ax2.hlines(box_sampling_input.box_depth_max, x_pts[0], xsec_length, colors = 'k')\n",
    "elif len(x_pts) == 2:\n",
    "    ax2.hlines(box_sampling_input.box_depth_min, x_pts[0], x_pts[1], colors = 'k')\n",
    "    ax2.hlines(box_sampling_input.box_depth_max, x_pts[0], x_pts[1], colors = 'k')\n",
    "\n",
    "fig.suptitle(suptitle, fontsize = 14);\n",
    "print(f'west: {w}, south: {s}, east: {e}, north: {n}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027449ce-a6f3-4c26-b6ce-08bbfd51e139",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 2: Filter dataset by individual measurement attributes\n",
    "\n",
    "#### **`box_sampling.py` must be run before proceeding!**\n",
    "\n",
    "In this part, the user can filter the dataset of individual measurements based on attributes including:\n",
    "\n",
    "- Station location (`STA_LAT`, `STA_LON`)\n",
    "- Any retained columns (e.g., quality weights) defined in `box_sampling_input.columns_to_keep`\n",
    "- Phase (`PHASE`)\n",
    "- Azimuth (`AZIMUTH`)\n",
    "- Bottoming depth (km) (`MAX_DEPTH_KM`)\n",
    "- Total length of raypaths (km) (`TOTAL_LENGTH_KM`)\n",
    "- Length of the portion of the raypaths that pass through the box (`LENGTH_IN_BOX_KM`)\n",
    "- Percent of the raypaths that pass through the box (`%_IN_BOX`)\n",
    "\n",
    "Event-related filters (i.e., event location (`EQ_LAT`, `EQ_LON`), magnitude (`EQ_MAG`), depth (`EQ_DEP`)) can be added now or in later steps (recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e0bbe-8af9-4d54-9e49-d8c7de653428",
   "metadata": {},
   "source": [
    "#### Define the dataset and print a preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111cf43-9493-4ada-85cb-30cfebdbc401",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(box_sampling_input.dataset_save_name)\n",
    "print(f'number of raypaths represented in the dataset before initial filtering: {len(df_data)}')\n",
    "print()\n",
    "display(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c790b-2d6b-479b-a065-6ebff55be03b",
   "metadata": {},
   "source": [
    "#### Interactively apply any filters to the dataframe:\n",
    "\n",
    "Note: this cell (as well as all future cells) creates a new copy of the dataframe everytime it's run, so i can be rerun it as many times as necessary to try new/different combinations of filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4aae0-589e-4f1d-ad96-83597c2392ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtered_data = df_data.copy()\n",
    "\n",
    "part1_headers = []\n",
    "part1_values = []\n",
    "part1_thresholds = []\n",
    "part1_remaining = []\n",
    "\n",
    "while True:\n",
    "    add_filter = str(input(f'Do you have a filter to add? [y / n]: '))\n",
    "    if add_filter == 'n':\n",
    "        break\n",
    "    \n",
    "    elif add_filter == 'y':\n",
    "        other_value_header = str(input(f'What is the header of the filter you would like to add?: '))\n",
    "        while other_value_header not in df_data.columns:\n",
    "            other_value_header = str(input(f'Sorry, that header is not in the dataframe. Is there another header you would like to try?: '))\n",
    "        \n",
    "        if other_value_header == 'PHASE':\n",
    "            other_value_filter = str(input(f'What phase or phases would you like to apply a filter to (format is either `phase` or `phase1, phase2,...`)?: '))\n",
    "            other_value_filter_type = str(input(f'Would you like to exclude {other_value_filter} or make a dataframe of raypaths that exclusively matches {other_value_filter}? [exclude / match]: '))\n",
    "            \n",
    "            if ', ' in other_value_filter:\n",
    "                other_value_filter = other_value_filter.split(', ')\n",
    "\n",
    "            if other_value_filter_type == 'exclude':\n",
    "                if type(other_value_filter) == str:\n",
    "                    df_filtered_data = df_filtered_data.loc[df_filtered_data[other_value_header] != other_value_filter]\n",
    "                elif type(other_value_filter) == list:\n",
    "                    df_filtered_data = df_filtered_data.loc[~df_filtered_data[other_value_header].isin(other_value_filter)]\n",
    "                    \n",
    "            elif other_value_filter_type == 'match':\n",
    "                if type(other_value_filter) == str:\n",
    "                    df_filtered_data = df_filtered_data.loc[df_filtered_data[other_value_header] == other_value_filter]\n",
    "                elif type(other_value_filter) == list:\n",
    "                    df_filtered_data = df_filtered_data.loc[df_filtered_data[other_value_header].isin(other_value_filter)]\n",
    "        \n",
    "        else:\n",
    "            other_value_filter = float(input(f'What is the value of the filter you would like to apply?: '))\n",
    "            other_value_filter_type = str(input(f'Is the given value a maximum or minimum threshold? [max / min]: '))\n",
    "            if other_value_filter_type == 'min':\n",
    "                df_filtered_data = df_filtered_data.loc[df_filtered_data[other_value_header] >= other_value_filter]\n",
    "            elif other_value_filter_type == 'max':\n",
    "                df_filtered_data = df_filtered_data.loc[df_filtered_data[other_value_header] <= other_value_filter]\n",
    "        \n",
    "        print(f'You have {len(df_filtered_data)} of {len(df_data)} raypaths in your filtered dataset')\n",
    "\n",
    "        part1_headers.append(other_value_header)\n",
    "        part1_values.append(other_value_filter)\n",
    "        part1_thresholds.append(other_value_filter_type)\n",
    "        part1_remaining.append(len(df_filtered_data))\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "df_filtered_data = df_filtered_data.reset_index(drop = True)\n",
    "print()\n",
    "print(f'You have {len(df_filtered_data)} of {len(df_data)} raypaths in your filtered dataset')\n",
    "display(df_filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6be95-367d-4374-b5b2-907ea8f7bb80",
   "metadata": {},
   "source": [
    "#### Display a dataframe showing the number of phases represented in the current filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0120c-0b94-407c-838b-3c09693f2667",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "unique_filtered_phases = df_filtered_data.PHASE.unique()\n",
    "display(df_filtered_data.groupby(['PHASE']).size().reset_index())\n",
    "total_unique_phases = len(unique_filtered_phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97972b16-c050-4da6-afe3-05c6910784d8",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 3: Filter dataset by unique event attributes and compile a prioritized event dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60834071-4951-4eb3-947d-99a752bfe124",
   "metadata": {},
   "source": [
    "Find all unique events to which the filtered raypaths belong and compute unique event attributes based on filtered raypaths.\n",
    "\n",
    "New attributes/filtering criteria include the following:\n",
    "\n",
    "- Event location (`EQ_LAT`, `EQ_LON`)\n",
    "- Event magnitude (`EQ_MAG`)\n",
    "- Event depth (`EQ_DEP`)\n",
    "- Total number of raypaths associated with each unique event (`RAYPATHS`)\n",
    "- Averages of any retained values from the input dataset to be considered (defined in `box_sampling_input.columns_to_keep`)\n",
    "- The range of azimuths from 0-180 degrees covered by the raypaths associated with each unique event (`AZ_RANGE`)\n",
    "- The mean of azimuthal spacing from 0-180 degrees between the raypaths associated with each unique event (`AZ_SPACING_MEAN`)\n",
    "- The standard deviation of azimuthal spacing from 0-180 degrees between the raypaths associated with each unique event (`AZ_SPACING_STD`)\n",
    "- The number of unique phases represented by the raypaths associated with each unique event (`UNIQUE_PHASES`)\n",
    "- The epicentral distance of each unique event from the geographical center of all unique events (`DIST_FROM_CENTER`)\n",
    "- The azimuth between the center and each unique event (`AZ_FROM_CENTER`)\n",
    "- The cumulative epicentral distance between each event and every other event (`CUMULATIVE_DIST`)\n",
    "- The cumulative azimuth between each event and every other event (`CUMULATIVE_AZ`)\n",
    "- The normalized sum of the normalized cumulative distance and cumulative azimuth to represent how dispersed each event is (`DISPERSION_SCORE`)\n",
    "\n",
    "#### Print a preview of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e48ae9-5f05-4326-bc56-d6649ce9fde5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_events = df_filtered_data['EVENT_ID'].unique()\n",
    "df_unique_events = pd.DataFrame(data = {'EVENT_ID': unique_events})\n",
    "df_unique_events['EQ_LAT'] = 0.\n",
    "df_unique_events['EQ_LON'] = 0.\n",
    "df_unique_events['EQ_MAG'] = 0\n",
    "df_unique_events['EQ_DEP'] = 0\n",
    "df_unique_events['RAYPATHS'] = 0\n",
    "\n",
    "for col in box_sampling_input.columns_to_keep:\n",
    "    df_unique_events[col] = 0.\n",
    "df_unique_events['AZ_RANGE'] = 0.\n",
    "df_unique_events['AZ_SPACING_MEAN'] = 0.\n",
    "df_unique_events['AZ_SPACING_STD'] = 0.\n",
    "df_unique_events['UNIQUE_PHASES'] = 0\n",
    "\n",
    "# for each event, add average weights from the input dataset\n",
    "for event in range(len(df_unique_events)):\n",
    "    event_id = df_unique_events['EVENT_ID'].iloc[event]\n",
    "    df_event = df_filtered_data.loc[df_filtered_data['EVENT_ID'] == event_id].copy()\n",
    "    df_unique_events.loc[event, 'EQ_MAG'] = df_event['EQ_MAG'].iloc[0]\n",
    "    df_unique_events.loc[event, 'EQ_DEP'] = df_event['EQ_DEP'].iloc[0]\n",
    "    df_unique_events.loc[event, 'UNIQUE_PHASES'] = len(df_event.PHASE.unique())\n",
    "    df_unique_events.loc[event, 'EQ_LAT'] = df_event['EQ_LAT'].iloc[0]\n",
    "    df_unique_events.loc[event, 'EQ_LON'] = df_event['EQ_LON'].iloc[0]\n",
    "    df_unique_events.loc[event, 'RAYPATHS'] = len(df_event)\n",
    "\n",
    "    for col in box_sampling_input.columns_to_keep:\n",
    "        df_unique_events.loc[event, col] = df_event[col].mean()\n",
    "    \n",
    "    if len(df_event) == 1:\n",
    "        df_unique_events.loc[event, 'AZ_RANGE'] = np.nan\n",
    "        df_unique_events.loc[event, 'AZ_SPACING_MEAN'] = np.nan\n",
    "        df_unique_events.loc[event, 'AZ_SPACING_STD'] = np.nan\n",
    "        \n",
    "    else:\n",
    "        sorted_az = df_event['AZIMUTH'].apply(convert_azimuth).sort_values()        \n",
    "        df_unique_events.loc[event, 'AZ_RANGE'] = sorted_az.max() - sorted_az.min()\n",
    "        df_unique_events.loc[event, 'AZ_SPACING_MEAN'] = sorted_az.diff()[1:].mean()\n",
    "        df_unique_events.loc[event, 'AZ_SPACING_STD'] = sorted_az.diff()[1:].std()\n",
    "\n",
    "\n",
    "cloud_center = find_cloud_center(df_unique_events['EQ_LAT'], df_unique_events['EQ_LON'])\n",
    "\n",
    "df_unique_events['DIST_FROM_CENTER'] = dist_from_point(cloud_center[0], cloud_center[1], df_unique_events['EQ_LAT'], df_unique_events['EQ_LON'])\n",
    "df_unique_events['AZ_FROM_CENTER'] = az_from_point(cloud_center[0], cloud_center[1], df_unique_events['EQ_LAT'], df_unique_events['EQ_LON'])\n",
    "df_unique_events['CUMULATIVE_DIST'] = 0\n",
    "df_unique_events['CUMULATIVE_AZ'] = 0\n",
    "\n",
    "for event in range(len(df_unique_events)):\n",
    "    eq_lat = df_unique_events['EQ_LAT'].iloc[event]\n",
    "    eq_lon = df_unique_events['EQ_LON'].iloc[event]\n",
    "    az_from_center = df_unique_events['AZ_FROM_CENTER'].iloc[event]\n",
    "\n",
    "    # find the cumulative epicentral distance to every other point\n",
    "    df_unique_events.loc[event, 'CUMULATIVE_DIST'] = np.sum(dist_from_point(eq_lat, eq_lon, df_unique_events['EQ_LAT'], df_unique_events['EQ_LON']))\n",
    "\n",
    "    # find the cumulative angle between this event, the cloud center, and every other event\n",
    "    df_unique_events.loc[event, 'CUMULATIVE_AZ'] = np.sum(az_diff(az_from_center, df_unique_events['AZ_FROM_CENTER']))\n",
    "\n",
    "df_unique_events['DISPERSION_SCORE'] = (df_unique_events['CUMULATIVE_DIST'] / df_unique_events['CUMULATIVE_DIST'].max()) + (df_unique_events['CUMULATIVE_AZ'] / df_unique_events['CUMULATIVE_AZ'].max())\n",
    "df_unique_events['DISPERSION_SCORE'] = df_unique_events['DISPERSION_SCORE'] / df_unique_events['DISPERSION_SCORE'].max()\n",
    "df_unique_events = df_unique_events.sort_values(by = ['DISPERSION_SCORE'], ascending = False).reset_index(drop = True)\n",
    "\n",
    "print(f'There are {len(df_unique_events)} unique events from the filtered raypaths')\n",
    "print()\n",
    "display(df_unique_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e446ad4-1258-453a-b20d-3d135e2e7ed9",
   "metadata": {},
   "source": [
    "#### Plot a histogram and corresponding map of events showing the distribution of any header per unique event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80f69f-9dea-4780-9f6c-5921b270e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_to_plot = 'UNIQUE_PHASES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69075327-99d3-4452-9cb5-4050461ebbfd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ascending_kw = True\n",
    "cm = 'viridis'\n",
    "\n",
    "df_unique_events = df_unique_events.sort_values(by = [header_to_plot], ascending = ascending_kw).reset_index(drop = True)\n",
    "\n",
    "fig = plt.figure(figsize = (12, 4), constrained_layout = True)\n",
    "ax0 = fig.add_subplot(1, 2, 1)#, aspect = 2)\n",
    "ax0.set_box_aspect(.5)\n",
    "ax0.hist(df_unique_events[header_to_plot], edgecolor = 'k')\n",
    "ax0.set_title(f'{header_to_plot} distribution', fontsize = 14)\n",
    "ax0.set_xlabel(f'{header_to_plot}')\n",
    "ax0.set_ylabel('# of earthquakes');\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 2, projection = ccrs.Robinson(central_longitude = map_center))\n",
    "ax1.set_global()\n",
    "ax1.coastlines(alpha = 0.4)\n",
    "ax1.scatter(cloud_center[1], cloud_center[0], c = 'orange', marker = '*', s = 120, edgecolor = 'k')\n",
    "ax1.add_patch(mpatches.Rectangle(xy = [w, s], width = box_sampling_input.width, height = box_sampling_input.height, edgecolor = 'black', fill = False, alpha = 1, transform = ccrs.PlateCarree()))\n",
    "ax1.set_title(f'Locations of events', fontsize = 14)\n",
    "im1 = ax1.scatter(df_unique_events['EQ_LON'], df_unique_events['EQ_LAT'], c = df_unique_events[header_to_plot], transform = ccrs.PlateCarree(), edgecolor = 'k', marker = 'o', s = 75, cmap = cm, alpha = 1, zorder = 2)\n",
    "cbar = fig.colorbar(im1, ax = [ax1], orientation = 'horizontal', shrink = 0.8, pad = -0.05)\n",
    "cbar.ax.set_xlabel(f'{header_to_plot}', size = 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db30b76-c5d6-4ae5-9217-f3365a55a7d1",
   "metadata": {},
   "source": [
    "#### Apply any initial filters before applying prioritization weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d925f-461c-438c-a5e7-9859e4225a14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtered_events = df_unique_events.copy()\n",
    "\n",
    "part2_headers = []\n",
    "part2_values = []\n",
    "part2_thresholds = []\n",
    "part2_remaining = []\n",
    "\n",
    "while True:\n",
    "    add_filter = str(input(f'Do you have a filter to add? [y / n]: '))\n",
    "    if add_filter == 'n':\n",
    "        break\n",
    "    \n",
    "    elif add_filter == 'y':\n",
    "        other_value_header = str(input(f'What is the header in the dataframe of unique events of the filter you would like to add?: '))\n",
    "        \n",
    "        while other_value_header not in df_filtered_events.columns:\n",
    "            other_value_header = str(input(f'Sorry, that header is not in the dataframe. Is there another header you would like to try?: '))\n",
    "            \n",
    "        other_value_filter = float(input(f'What is the value of the filter you would like to apply?: '))\n",
    "        other_value_filter_type = str(input(f'Is the given value a maximum or minimum threshold? [max / min]: '))\n",
    "        if other_value_filter_type == 'min':\n",
    "            df_filtered_events = df_filtered_events.loc[df_filtered_events[other_value_header] >= other_value_filter]\n",
    "        elif other_value_filter_type == 'max':\n",
    "            df_filtered_events = df_filtered_events.loc[df_filtered_events[other_value_header] <= other_value_filter]\n",
    "\n",
    "        print(f'You have {len(df_filtered_events)} of {len(df_unique_events)} events in your filtered dataset')\n",
    "\n",
    "        part2_headers.append(other_value_header)\n",
    "        part2_values.append(other_value_filter)\n",
    "        part2_thresholds.append(other_value_filter_type)\n",
    "        part2_remaining.append(len(df_filtered_events))\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "df_filtered_events = df_filtered_events.reset_index(drop = True)\n",
    "print()\n",
    "print(f'You have {len(df_filtered_events)} of {len(df_unique_events)} events in your filtered dataset')\n",
    "display(df_filtered_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7312e50-f0df-49c5-b21c-a72e1e425689",
   "metadata": {},
   "source": [
    "#### Plot remaining filtered events based on any header in the filtered dataset of unique events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068b2ca-3cd4-4ce5-b431-e48e84e86fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_header_to_plot = 'EQ_MAG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fbe5c-3034-4e25-beb0-d6a8124c0a4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtered_events = df_filtered_events.sort_values(by = [filtered_header_to_plot], ascending = True).reset_index(drop = True)\n",
    "\n",
    "fig = plt.figure(figsize = (6, 4), constrained_layout = True)\n",
    "\n",
    "cm = 'viridis'\n",
    "ax1 = fig.add_subplot(1, 1, 1, projection = ccrs.Robinson(central_longitude = map_center))\n",
    "ax1.set_global()\n",
    "ax1.coastlines(alpha = 0.4)\n",
    "ax1.add_patch(mpatches.Rectangle(xy = [w, s], width = box_sampling_input.width, height = box_sampling_input.height, edgecolor = 'black', fill = False, alpha = 1, transform = ccrs.PlateCarree()))\n",
    "im1 = ax1.scatter(df_filtered_events['EQ_LON'], df_filtered_events['EQ_LAT'], c = df_filtered_events[filtered_header_to_plot], transform = ccrs.PlateCarree(), edgecolor = 'k', marker = 'o', s = 75, cmap = cm, alpha = 1, zorder = 2)\n",
    "cbar = fig.colorbar(im1, ax = [ax1], orientation = 'horizontal', shrink = 0.8, pad = 0.1)\n",
    "cbar.ax.set_xlabel(f'{filtered_header_to_plot}', size = 12)\n",
    "\n",
    "fig.suptitle(f'Locations of filtered events', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741ba76-f4f4-489c-8ac1-10832d623b11",
   "metadata": {},
   "source": [
    "#### Finally, apply priority weights to order the data based on unique event attributes:\n",
    "\n",
    "In this step, priority weights can be added for any header. Weight values can be added between 0 and 1, where 0 indicates a weight that is negligible and 1 indicates a weight that is top priority. For example, if the number of raypaths per event is extremely important, a priority weight of 1 would be given for the header `RAYPATHS`. However, if the number of unique phases is _also_ important, but perhaps not as important as the number of raypaths, a priority weight of 0.5 could be assigned to the header `UNIQUE_PHASES`. If no weight is given to a header, its values are not considered in reordering/reprioritizing the dataframe of unique events. In this way, the unique events can be ordered according to different combinations of weights and attributes to find an optimal set of events.\n",
    "\n",
    "Once all prioritization weights are applied, there will be an option for applying a cutoff for the total number of events desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02582088-a1fe-4984-b4a9-08eea55bd469",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prioritized = df_filtered_events.copy()\n",
    "df_prioritized['PRIORITY'] = 0.\n",
    "\n",
    "part3_headers = []\n",
    "part3_favors = []\n",
    "part3_weights = []\n",
    "\n",
    "while True:\n",
    "    add_headers = str(input(f'Do you want to add a priority to the filtered event dataframe? [y / n]: '))\n",
    "    \n",
    "    if add_headers == 'y':\n",
    "        priority_header = str(input(f'What is the header of the column you would like to prioritize by?: '))\n",
    "        priority_favor = str(input(f'Would you like to prioritize high or low values? [high / low]: '))\n",
    "        priority_weight = float(input(f'What weight would you like to use for this priority? [0 < weight <= 1]: '))\n",
    "                \n",
    "        if priority_favor == 'high':\n",
    "            weighted_priority = (df_prioritized[priority_header] / df_prioritized[priority_header].max()) * priority_weight\n",
    "        elif priority_favor == 'low':\n",
    "            weighted_priority = (df_prioritized[priority_header].min() / df_prioritized[priority_header]) * priority_weight\n",
    "            \n",
    "        df_prioritized['PRIORITY'] += weighted_priority\n",
    "\n",
    "        part3_headers.append(priority_header)\n",
    "        part3_favors.append(priority_favor)\n",
    "        part3_weights.append(priority_weight)\n",
    "        \n",
    "    elif add_headers == 'n':\n",
    "        break\n",
    "    print()\n",
    "\n",
    "df_prioritized['PRIORITY'] = df_prioritized['PRIORITY'] / df_prioritized['PRIORITY'].max()\n",
    "df_prioritized = df_prioritized.sort_values(by = ['PRIORITY'], ascending = False).reset_index(drop = True)\n",
    "\n",
    "print()\n",
    "add_cutoff = str(input(f'Would you like to apply a cutoff for the total number of events to include in the final prioritized list? [y / n]: '))\n",
    "if add_cutoff == 'y':\n",
    "    cutoff_max = int(input(f'How many total events would you like to include in the final list?: '))\n",
    "    df_prioritized = df_prioritized.iloc[:cutoff_max]\n",
    "\n",
    "df_prioritized_raypaths = df_filtered_data.loc[df_filtered_data['EVENT_ID'].isin(df_prioritized['EVENT_ID'])]\n",
    "display(df_prioritized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abb7d2-fd0c-43eb-9580-f7d95195db31",
   "metadata": {},
   "source": [
    "#### Plot the filtered events with respect to their new priority score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a853770-1417-4cac-887b-6708bf392f16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prioritized = df_prioritized.sort_values(by = ['PRIORITY'], ascending = True).reset_index(drop = True)\n",
    "fig = plt.figure(figsize = (6, 4), constrained_layout = True)\n",
    "\n",
    "ax1 = fig.add_subplot(1, 1, 1, projection = ccrs.Robinson(central_longitude = map_center))\n",
    "ax1.set_global()\n",
    "ax1.coastlines(alpha = 0.4)\n",
    "ax1.add_patch(mpatches.Rectangle(xy = [w, s], width = box_sampling_input.width, height = box_sampling_input.height, edgecolor = 'black', fill = False, alpha = 1, transform = ccrs.PlateCarree()))\n",
    "cm = 'plasma'\n",
    "im1 = ax1.scatter(df_prioritized['EQ_LON'], df_prioritized['EQ_LAT'], c = df_prioritized['PRIORITY'], transform = ccrs.PlateCarree(), edgecolor = 'k', marker = 'o', s = 75, cmap = cm, alpha = 1, zorder = 2)\n",
    "cbar = fig.colorbar(im1, ax = [ax1], orientation = 'horizontal', shrink = 0.8, pad = 0.1)\n",
    "cbar.ax.set_xlabel('Priority score', size = 12)\n",
    "\n",
    "fig.suptitle(f'Locations of prioritized events', fontsize = 14)\n",
    "fig.savefig('./tech_review_plots/access_CH_priority_events.png', dpi = 300, transparent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3d0d7-be20-4e38-84f9-eaa48be8efcd",
   "metadata": {},
   "source": [
    "#### Plot the total and azimuthal coverage of the prioritized list of events:\n",
    "\n",
    "First, compute the gridded total and azimuthal sampling (note, this can take a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be436c-95ca-4ede-adca-33f341050586",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "block_height = 2\n",
    "df_grid = make_equal_area_grid(block_height, s, w, n, e)\n",
    "ar_grid = np.array(df_grid)\n",
    "\n",
    "df_grid_count = df_grid[['BLOCK#']].copy()\n",
    "df_grid_count['TOTAL_PATHS'] = 0\n",
    "df_grid_count['TOTAL_SECTORS'] = 0\n",
    "df_grid_count['SECTOR_1'] = 0\n",
    "df_grid_count['SECTOR_2'] = 0\n",
    "df_grid_count['SECTOR_3'] = 0\n",
    "df_grid_count['SECTOR_4'] = 0\n",
    "df_grid_count['SECTOR_5'] = 0\n",
    "df_grid_count['SECTOR_6'] = 0\n",
    "\n",
    "ar_grid_count = np.array(df_grid_count)\n",
    "ar_prioritized = np.array(df_prioritized)\n",
    "\n",
    "event_ct = 1\n",
    "for event in ar_prioritized[:, 0]:\n",
    "    event_ct += 1\n",
    "    ar_event =  np.loadtxt(f'./event_files/{event}.csv', skiprows = 1, delimiter = ',', usecols = (3, 4, 7, 8))\n",
    "    \n",
    "    for line in range(len(ar_event)):\n",
    "        entry_lat = ar_event[line, 0]\n",
    "        entry_lon = ar_event[line, 1]\n",
    "        exit_lat = ar_event[line, 2]\n",
    "        exit_lon = ar_event[line, 3]\n",
    "        dist = geo_math.GCP_length(entry_lat, entry_lon, exit_lat, exit_lon)\n",
    "        segment_blocks = []\n",
    "        \n",
    "        progress = 0\n",
    "        run = True\n",
    "        while run == True:\n",
    "            progress += 1\n",
    "            if progress < dist:\n",
    "                seg_mid_point = geo_math.GCP_point(entry_lat, entry_lon, exit_lat, exit_lon, dist, progress / 2)\n",
    "                seg_mid_point_block = find_block_id(seg_mid_point[0], seg_mid_point[1], ar_grid)\n",
    "                seg_mid_point_block_id = np.where(ar_grid_count.T[0] == seg_mid_point_block)[0]\n",
    "                \n",
    "                if seg_mid_point_block_id not in segment_blocks:\n",
    "                    seg_mid_point_az = geo_math.azimuth(seg_mid_point[0], seg_mid_point[1], exit_lat, exit_lon)\n",
    "                    seg_mid_point_sector = azimuthal_sector(seg_mid_point_az)\n",
    "                    ar_grid_count[seg_mid_point_block_id, 1] += 1\n",
    "                    ar_grid_count[seg_mid_point_block_id, seg_mid_point_sector + 2] += 1\n",
    "                    segment_blocks.append(seg_mid_point_block_id)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                run = False\n",
    "\n",
    "for count in range(len(ar_grid_count)):\n",
    "    sector_array = ar_grid_count[count, 3:]\n",
    "    sector_count = np.count_nonzero(sector_array)\n",
    "    ar_grid_count[count, 2] = sector_count\n",
    "\n",
    "df_grid_count = pd.DataFrame(ar_grid_count, columns = df_grid_count.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e966a7-1220-4b64-b08a-04a9bbc6924f",
   "metadata": {},
   "source": [
    "Once the sampling is computed, plot the coverage maps. Total coverage is defined as the number of paths that sample a given grid area. Azimuthal coverage is defined as the number of azimuthal sectors (defined as six 30 degree sectors from 0 - 180 degrees) with coverage in a given grid area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fdd52-883f-409a-a6b4-4ceff762289d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 4), constrained_layout = True)\n",
    "box_lats = np.arange(s, n + block_height, block_height)\n",
    "box_lons = np.arange(0, box_sampling_input.width + block_height, block_height)\n",
    "box_lons = box_lons + w\n",
    "box_lons[np.where(box_lons >= 180)[0]] = box_lons[np.where(box_lons >= 180)[0]] - 360\n",
    "\n",
    "## total coverage:\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection = ccrs.Robinson(central_longitude = map_center))\n",
    "ax1.set_global()\n",
    "ax1.coastlines(alpha = 0.4)\n",
    "\n",
    "cov_bins = [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "total_paths = coverage_mesh(ar_grid, ar_grid_count, block_height, 1)\n",
    "cov_norm = mpl.colors.BoundaryNorm(cov_bins, mpl.cm.Oranges.N)\n",
    "im1 = ax1.pcolormesh(box_lons, box_lats, total_paths, transform = ccrs.PlateCarree(), cmap = 'Oranges', norm = cov_norm)\n",
    "ax1.set_title(f'Total coverage per block', fontsize = 14)\n",
    "ax1.add_patch(mpatches.Rectangle(xy = [w, s], width = box_sampling_input.width, height = box_sampling_input.height, edgecolor = 'k', fill = False, alpha = 1, transform = ccrs.PlateCarree(), linewidth = 1))\n",
    "\n",
    "cov_cbar_coverage = fig.colorbar(im1, ax = ax1, orientation = 'horizontal', shrink = 0.8, pad = 0.1, extend = 'max')\n",
    "cov_cbar_coverage.ax.tick_params(labelsize = 10)\n",
    "cov_cbar_coverage.ax.set_xlabel('# of paths', size = 12)\n",
    "\n",
    "## azimuthal coverage:\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection = ccrs.Robinson(central_longitude = map_center))\n",
    "ax2.set_global()\n",
    "ax2.coastlines(alpha = 0.4)\n",
    "\n",
    "sector_bins = list(range(0, azimuthal_sectors + 2, 1))\n",
    "total_sectors = coverage_mesh(ar_grid, ar_grid_count, block_height, 2)\n",
    "sector_norm = mpl.colors.BoundaryNorm(sector_bins, mpl.cm.Purples.N)\n",
    "im2 = ax2.pcolormesh(box_lons, box_lats, total_sectors, transform = ccrs.PlateCarree(), cmap = 'Purples', norm = sector_norm)\n",
    "ax2.set_title(f'Azimuthal coverage per block', fontsize = 14)\n",
    "ax2.add_patch(mpatches.Rectangle(xy = [w, s], width = box_sampling_input.width, height = box_sampling_input.height, edgecolor = 'k', fill = False, alpha = 1, transform = ccrs.PlateCarree(), linewidth = 1))\n",
    "\n",
    "sector_cbar_coverage = fig.colorbar(im2, ax = ax2, orientation = 'horizontal', shrink = 0.8, pad = 0.1, extend = 'neither')\n",
    "sector_cbar_coverage.set_ticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5])\n",
    "sector_cbar_coverage.set_ticklabels(sector_bins[:-1])\n",
    "sector_cbar_coverage.ax.tick_params(labelsize = 10, size = 0)\n",
    "sector_cbar_coverage.ax.set_xlabel('# of sectors', size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59e69e-3e57-4043-b023-f555438b45bc",
   "metadata": {},
   "source": [
    "#### Finally, indicate if you would like to retain this list of events and filtering/prioritization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec5a23-47a6-4691-9fa3-2c72df9d0133",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query for whether to save files:\n",
    "save_files = str(input(f'Would you like to save this list of events and associated filters/weights? [y / n]: '))\n",
    "\n",
    "if save_files == 'y':\n",
    "    filename = str(input(f'Give your list a short name with no spaces to be used for the file name (e.g., list_for_hawaii_model): '))\n",
    "    try:\n",
    "        os.mkdir(f'./events_lists')\n",
    "    except:\n",
    "        pass\n",
    "    df_prioritized.to_csv(f'./events_lists/{filename}_events.csv', index = False)\n",
    "    df_prioritized_raypaths.to_csv(f'./events_lists/{filename}_raypaths.csv', index = False)\n",
    "    with open(f'./events_lists/{filename}_params.txt', 'w') as fout:\n",
    "        fout.write(f'====================\\n')\n",
    "        fout.write(f'Input file parameters:\\n')\n",
    "        fout.write(f'Box width (lon. extent): {box_sampling_input.width}\\n')\n",
    "        fout.write(f'Box height (lat. extent): {box_sampling_input.height}\\n')\n",
    "        fout.write(f'Southernmost box latitude: {box_sampling_input.south}\\n')\n",
    "        fout.write(f'Westernmost box longitude: {box_sampling_input.west}\\n')\n",
    "        fout.write(f'Box minimum depth: {box_sampling_input.box_depth_min}\\n')\n",
    "        fout.write(f'Box maximum depth: {box_sampling_input.box_depth_max}\\n')\n",
    "        fout.write(f'Input raypath dataset name: {box_sampling_input.dataset_input_name}\\n')\n",
    "        fout.write(f'Input raypath dataset columns retained: {box_sampling_input.columns_to_keep}\\n')\n",
    "        fout.write(f'Output raypath dataset name: {box_sampling_input.dataset_save_name}\\n')\n",
    "        \n",
    "        fout.write(f'\\n====================\\n')\n",
    "        fout.write(f'Plotting parameters:\\n')\n",
    "        fout.write(f'Model background: {model}\\n')\n",
    "        if model != None:\n",
    "            fout.write(f'Model wave type: {model_wave_type}\\n')\n",
    "            fout.write(f'Model depth: {model_depth}\\n')\n",
    "            fout.write(f'Model value: {model_value}\\n')\n",
    "            fout.write(f'Model value type: {model_value_type}\\n')\n",
    "            fout.write(f'Map increment: {map_increment}\\n')\n",
    "            fout.write(f'Colorbar min/max value: {tomo_color_bar}\\n')\n",
    "        fout.write(f'Cross-section start point: {start_point}\\n')\n",
    "        fout.write(f'Cross-section end point: {end_point}\\n')\n",
    "        fout.write(f'Number of reference points in cross-section: {number_of_reference_points}\\n')\n",
    "        \n",
    "        fout.write(f'\\n====================\\n')\n",
    "        fout.write(f'Part 1 - Individual raypath filtering:\\n')\n",
    "        for filter in range(len(part1_headers)):\n",
    "            fout.write(f'Filter # {filter + 1}:\\n')\n",
    "            fout.write(f'  Filter header: {part1_headers[filter]}\\n')\n",
    "            fout.write(f'  Filter value: {part1_values[filter]}\\n')\n",
    "            fout.write(f'  Filter type: {part1_thresholds[filter]}\\n')\n",
    "            fout.write(f'  Remaining raypaths after filter: {part1_remaining[filter]} of {len(df_data)}\\n')\n",
    "\n",
    "        fout.write(f'\\n====================\\n')\n",
    "        fout.write(f'Part 2 - Unique event filtering:\\n')\n",
    "        for filter in range(len(part2_headers)):\n",
    "            fout.write(f'Filter # {filter + 1}:\\n')\n",
    "            fout.write(f'  Filter header: {part2_headers[filter]}\\n')\n",
    "            fout.write(f'  Filter value: {part2_values[filter]}\\n')\n",
    "            fout.write(f'  Filter type: {part2_thresholds[filter]}\\n')\n",
    "            fout.write(f'  Remaining events after filter: {part2_remaining[filter]} of {len(df_data)}\\n')\n",
    "\n",
    "        fout.write(f'\\n====================\\n')\n",
    "        fout.write(f'Part 3 - Priority weights:\\n')\n",
    "        for filter in range(len(part3_headers)):\n",
    "            fout.write(f'Priority weight # {filter + 1}:\\n')\n",
    "            fout.write(f'  Prioritized value header: {part3_headers[filter]}\\n')\n",
    "            fout.write(f'  Prioritized value favor: {part3_favors[filter]}\\n')\n",
    "            fout.write(f'  Priority weight: {part3_weights[filter]}\\n')\n",
    "\n",
    "elif save_files == 'n':\n",
    "    print('List of prioritized events and associated filters/weights not saved.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ceac3-54dc-4046-a59e-cf02329e2133",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sitrus",
   "language": "python",
   "name": "sitrus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
